#!/usr/bin/env python3

from ray.experimental.parallel_ml.coordinator import Coordinator
from ray.experimental.parallel_ml.physical_plan import ModuleParition, SimplePhysicalPlanner
from ray.util.placement_group import placement_group

from torch import nn
import torch

"""
Notes:
* layer_past is an optimization for decoder-only transformer models
    only the query token is updated each block, so can share previous k,v to optimize
    for pipline parallel probably should disable this, not sure if it's worth communication cost.
* Need to unravel this loop
    https://github.com/huggingface/transformers/blob/v4.27.2/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L600
    I think the first and last ones are easy; just replace self.h with a sublist.
    the intermediate ones need something else.. I think for many transformer models since it's literally just
    sequential blocks, we can get away with re-implementing the block call ourselves..
"""

# taken from set_module_tensor_to_device, but works for name pointing to element of ModuleList
def resolve_module(module, name):
    for split in name.split("."):
        module = getattr(module, split)
        assert module is not None, f"{name} error on {split}"
    return module

def gen_logical_plan():
    pass

if True:
    
    import subprocess
    #subprocess.check_call("pip install -U accelerate 'numpy<1.24' transformers", shell=True)
    from accelerate import init_empty_weights, load_checkpoint_and_dispatch, infer_auto_device_map, dispatch_model
    from transformers import AutoConfig, AutoModelForCausalLM
    checkpoint = "EleutherAI/gpt-neo-125m"
    config = AutoConfig.from_pretrained(checkpoint)
    #with init_empty_weights():
    #    model = AutoModelForCausalLM.from_config(config)
    #    model = model.eval()
    #    device_map = infer_auto_device_map(model)
    #    #print(model)
    #    #print(device_map)

    #stack = list(model.named_children())
    #zero = None
    #while stack:
    #    name, module = stack.pop()
    #    print(name)

    #    if name in ['transformer', 'h', '0']:
    #        stack.extend(list(module.named_children()))
    #        if name == '0':
    #            zero = module

    device_map = {
        "transformer": "cpu",
        "lm_head": "cpu",
    }

    model = AutoModelForCausalLM.from_config(config)
    model = dispatch_model(model, device_map=device_map)

    #print(model)


    from accelerate.utils.modeling import set_module_tensor_to_device
    #getattr(getattr(model, 'transformer'), 'h')

    embedding = resolve_module(model, 'transformer.wte')
    blocks = resolve_module(model, 'transformer.h')
    print(blocks)
    
    #set_module_tensor_to_device(model, 'transformer.h.0.a', 'cuda:0')

    #out = ModuleParition(
    #    partition_index=0,
    #    module=None,
    #    module_loader=lambda: None,
    #    input_tensor_shape=(1, 2),
    #    input_tensor_dtype=torch.float16,
    #)

if __name__ == '__main__':
    pass
    #physical_planner = SimplePhysicalPlanner()
    #placement_group = placement_group([{'GPU': 1}, {'GPU': 1}], strategy='STRICT_PACK')
    #logical_plan = gen_logical_plan()
    #
    #coordinator = Coordinator(
    #    logical_plan=logical_plan,
    #    pg=placement_group,
    #    planner=physical_planner,
    #)
