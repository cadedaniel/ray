#!/usr/bin/env python3

from ray.experimental.parallel_ml.coordinator import Coordinator
from ray.experimental.parallel_ml.physical_plan import ModuleParition, SimplePhysicalPlanner
from ray.util.placement_group import placement_group

import subprocess
#subprocess.check_call("pip install -U accelerate 'numpy<1.24' transformers", shell=True)
from accelerate import init_empty_weights, load_checkpoint_and_dispatch, infer_auto_device_map, dispatch_model
from accelerate.utils.modeling import set_module_tensor_to_device
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, GPTNeoForCausalLM
from transformers.generation.logits_process import LogitsProcessorList, LogitNormalization

from torch import nn
import torch

"""
Notes:
* layer_past is an optimization for decoder-only transformer models
    only the query token is updated each block, so can share previous k,v to optimize
    for pipline parallel probably should disable this, not sure if it's worth communication cost.
* Need to unravel this loop
    https://github.com/huggingface/transformers/blob/v4.27.2/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L600
    I think the first and last ones are easy; just replace self.h with a sublist.
    the intermediate ones need something else.. I think for many transformer models since it's literally just
    sequential blocks, we can get away with re-implementing the block call ourselves..
"""

# taken from set_module_tensor_to_device, but works for name pointing to element of ModuleList
def resolve_module(module, name):
    for split in name.split("."):
        module = getattr(module, split)
        assert module is not None, f"{name} error on {split}"
    return module

def gen_logical_plan():
    pass

def custom_forward_pass_pp0(self, *args, **kwargs):
    output_attentions = False
    output_hidden_states = False
    use_cache = False
    return_dict = False
    inputs_embeds = None
    attention_mask = None
    head_mask = None

    input_ids = kwargs.pop('input_ids')
    input_shape = input_ids.size()
    input_ids = input_ids.view(-1, input_shape[-1])
    batch_size = input_ids.shape[0]
    device = input_ids.device

    # hack, didn't realize the class was GPTNeoForCausalLM
    self = self.transformer

    past_length = 0
    past_key_values = tuple([None] * len(self.h))

    position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)
    position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])

    head_mask = self.get_head_mask(head_mask, self.config.num_layers)

    if inputs_embeds is None:
        inputs_embeds = self.wte(input_ids)
    position_embeds = self.wpe(position_ids)
    hidden_states = inputs_embeds + position_embeds

    # TODO(cade) not sure what this does
    hidden_states = self.drop(hidden_states)

    # Not used in model_start, but need it later.
    output_shape = input_shape + (hidden_states.size(-1),)
    
    presents = () if use_cache else None
    all_self_attentions = () if output_attentions else None
    all_hidden_states = () if output_hidden_states else None
    for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):
        outputs = block(
            hidden_states,
            layer_past=layer_past,
            attention_mask=attention_mask,
            head_mask=head_mask[i],
            use_cache=use_cache,
            output_attentions=output_attentions,
        )
    
        hidden_states = outputs[0]
    return hidden_states, output_shape


def custom_forward_pass_pp1(self, *args, **kwargs):
    output_attentions = False
    output_hidden_states = False
    use_cache = False
    return_dict = False
    inputs_embeds = None
    attention_mask = None
    head_mask = None

    hidden_states = kwargs.pop('hidden_states')
    output_shape = kwargs.pop('output_shape')

    # hack, didn't realize the class was GPTNeoForCausalLM
    self = self.transformer

    past_length = 0
    past_key_values = tuple([None] * len(self.h))

    head_mask = self.get_head_mask(head_mask, self.config.num_layers)

    for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):
        outputs = block(
            hidden_states,
            layer_past=layer_past,
            attention_mask=attention_mask,
            head_mask=head_mask[i],
            use_cache=use_cache,
            output_attentions=output_attentions,
        )
    
        hidden_states = outputs[0]

    hidden_states = self.ln_f(hidden_states)

    # TODO need to calculate output shape
    hidden_states = hidden_states.view(output_shape)
    return hidden_states

custom_fwd_pass = [custom_forward_pass_pp0, custom_forward_pass_pp1]

def replace_forward_pass(gpt_neo_model, pp_rank):
    assert type(gpt_neo_model) == GPTNeoForCausalLM
    gpt_neo_model.forward = custom_fwd_pass[pp_rank].__get__(
        gpt_neo_model,
        gpt_neo_model.__class__,
    )

#from transformers.models.gpt_neo import GPTNeoModel
#class PP_GPTNeoModel(GPTNeoModel):
#    def __init__(self, config):
#        super().__init__(config)

if True:
    
    checkpoint = "EleutherAI/gpt-neo-125m"
    config = AutoConfig.from_pretrained(checkpoint)
    #with init_empty_weights():
    #    model = AutoModelForCausalLM.from_config(config)
    #    model = model.eval()
    #    device_map = infer_auto_device_map(model)
    #    #print(model)
    #    #print(device_map)

    #stack = list(model.named_children())
    #zero = None
    #while stack:
    #    name, module = stack.pop()
    #    print(name)

    #    if name in ['transformer', 'h', '0']:
    #        stack.extend(list(module.named_children()))
    #        if name == '0':
    #            zero = module

    #device_map = {
    #    "transformer": "cpu",
    #    "lm_head": "cpu",
    #}

    #model = AutoModelForCausalLM.from_config(config)
    #model = dispatch_model(model, device_map=device_map)

    #from accelerate.utils.modeling import set_module_tensor_to_device

    #embedding = resolve_module(model, 'transformer.wte')
    #blocks = resolve_module(model, 'transformer.h')
    #print(blocks)

    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    input_sequence = "The quick brown fox jumps over the"
    input_tokens = tokenizer(input_sequence, return_tensors="pt")
    input_ids = input_tokens['input_ids']

    device_map = {
        "transformer": "cpu",
        "lm_head": "cpu",
    }

    model_start = AutoModelForCausalLM.from_config(config)
    model_start = dispatch_model(model_start, device_map=device_map)

    model_end = AutoModelForCausalLM.from_config(config)
    model_end = dispatch_model(model_end, device_map=device_map)

    #blocks = resolve_module(model_start, 'transformer.h')
    #num_blocks = len(blocks)
    
    replace_forward_pass(model_start, pp_rank=0)
    replace_forward_pass(model_end, pp_rank=1)

    hidden_states, output_shape = model_start.forward(
        input_ids=input_ids,
        return_dict=False,
        output_attentions=False,
        output_hidden_states=False,
    )
    print('start got output', hidden_states, output_shape)

    end_output = model_end.forward(
        hidden_states=hidden_states,
        output_shape=output_shape,
    )
    print('end got output', end_output)

    lm_head_output = model_end.lm_head(end_output)
    lm_logits = lm_head_output
    print('lm_head_output ', lm_head_output)

    next_token_logits = lm_logits[:, -1, :]

    processors = LogitsProcessorList()
    processors.append(LogitNormalization())

    next_token_scores = processors(input_ids, next_token_logits)
    #next_token_scores = logits_warper(input_ids, next_token_scores) # skipping
    probs = nn.functional.softmax(next_token_scores, dim=-1)
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
    decoded = tokenizer.decode(next_tokens)
    print(f"'{input_sequence}' ->")
    print(f"'{input_sequence}{decoded}'",)

    """
    logits_processor = self._get_logits_processor(
        generation_config=generation_config,
        input_ids_seq_length=input_ids_seq_length,
        encoder_input_ids=inputs_tensor,
        prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,
        logits_processor=logits_processor,
    )
    """
    


    #num_pp_stages = 2
    #blocks_start = blocks[:num_blocks//num_pp_stages]
    #blocks_end = blocks[-num_blocks//num_pp_stages:]

    #from transformers.models.gpt_neo import GPTNeoModel
    ## GPTNeoModel

    #embedding = resolve_module(model, 'transformer.wte')
    #blocks = resolve_module(model, 'transformer.h')
    #print(blocks)
    

if __name__ == '__main__':
    pass
    #physical_planner = SimplePhysicalPlanner()
    #placement_group = placement_group([{'GPU': 1}, {'GPU': 1}], strategy='STRICT_PACK')
    #logical_plan = gen_logical_plan()
    #
    #coordinator = Coordinator(
    #    logical_plan=logical_plan,
    #    pg=placement_group,
    #    planner=physical_planner,
    #)
