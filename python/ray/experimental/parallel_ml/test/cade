#!/usr/bin/env python3

# Must be called before import ray
import subprocess
#subprocess.check_call("pip install -U accelerate 'numpy<1.24' transformers", shell=True)

from ray.experimental.parallel_ml.coordinator import Coordinator
from ray.experimental.parallel_ml.physical_plan import ModuleParition, SimplePhysicalPlanner
from ray.util.placement_group import placement_group

from collections import defaultdict
import functools
import copy

from accelerate import init_empty_weights, load_checkpoint_and_dispatch, infer_auto_device_map, dispatch_model
from accelerate.utils.modeling import set_module_tensor_to_device
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, GPTNeoForCausalLM
from transformers.generation.logits_process import LogitsProcessorList, LogitNormalization

from torch import nn
import torch

"""
Notes:
* layer_past is an optimization for decoder-only transformer models
    only the query token is updated each block, so can share previous k,v to optimize
    for pipline parallel probably should disable this, not sure if it's worth communication cost.
* Need to unravel this loop
    https://github.com/huggingface/transformers/blob/v4.27.2/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L600
    I think the first and last ones are easy; just replace self.h with a sublist.
    the intermediate ones need something else.. I think for many transformer models since it's literally just
    sequential blocks, we can get away with re-implementing the block call ourselves..
"""

# taken from set_module_tensor_to_device, but works for name pointing to element of ModuleList
def resolve_module(module, name):
    for split in name.split("."):
        module = getattr(module, split)
        assert module is not None, f"{name} error on {split}"
    return module

def gen_logical_plan():
    pass

def extract_blocks(h, block_indices, verbose=False):
    new_h = torch.nn.ModuleList()
    unused_h = torch.nn.ModuleList()

    for i, block in enumerate(h):
        if verbose:
            print(i, block_indices)
        if i in block_indices:
            new_h.append(block)
        else:
            unused_h.append(block)

    return new_h, unused_h

def remove_blocks_from_model(transformer, _lightrails_blocks):
    old_h = transformer.h
    transformer.h, unused_h = extract_blocks(transformer.h, _lightrails_blocks)
    # TODO delete unused unused_h

def custom_forward_pass_first_pp(self, *args, **kwargs):
    print('custom_forward_pass_first_pp')
    output_attentions = False
    output_hidden_states = False
    use_cache = False
    return_dict = False
    inputs_embeds = None
    attention_mask = None
    head_mask = None

    # hack, didn't realize the class was GPTNeoForCausalLM
    self = self.transformer

    #_lightrails_blocks = kwargs.pop("_lightrails_blocks")
    #old_h = self.h
    #self.h, unused_h = extract_blocks(old_h, _lightrails_blocks)
    ## TODO delete unused unused_h
    ## TODO cache model modification
    #print(f'custom_forward_pass_first_pp blocks: {len(self.h)}')

    input_ids = kwargs.pop('input_ids')
    input_shape = input_ids.size()
    input_ids = input_ids.view(-1, input_shape[-1])
    batch_size = input_ids.shape[0]
    device = input_ids.device

    past_length = 0
    past_key_values = tuple([None] * len(self.h))

    position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)
    position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])

    head_mask = self.get_head_mask(head_mask, self.config.num_layers)

    if inputs_embeds is None:
        inputs_embeds = self.wte(input_ids)
    position_embeds = self.wpe(position_ids)
    hidden_states = inputs_embeds + position_embeds

    # TODO(cade) not sure what this does
    hidden_states = self.drop(hidden_states)

    # Not used in model_start, but need it later.
    output_shape = input_shape + (hidden_states.size(-1),)
    
    presents = () if use_cache else None
    all_self_attentions = () if output_attentions else None
    all_hidden_states = () if output_hidden_states else None
    for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):
        outputs = block(
            hidden_states,
            layer_past=layer_past,
            attention_mask=attention_mask,
            head_mask=head_mask[i],
            use_cache=use_cache,
            output_attentions=output_attentions,
        )
    
        hidden_states = outputs[0]
    return hidden_states, output_shape


def custom_forward_pass_last_pp(self, *args, **kwargs):
    print('custom_forward_pass_last_pp')
    output_attentions = False
    output_hidden_states = False
    use_cache = False
    return_dict = False
    inputs_embeds = None
    attention_mask = None
    head_mask = None

    hidden_states = kwargs.pop('hidden_states')
    output_shape = kwargs.pop('output_shape')

    # hack, didn't realize the class was GPTNeoForCausalLM
    lm_head = self.lm_head
    self = self.transformer

    #_lightrails_blocks = kwargs.pop("_lightrails_blocks")
    #old_h = self.h
    #self.h, unused_h = extract_blocks(old_h, _lightrails_blocks, verbose=True)
    ## TODO delete unused unused_h
    ## TODO cache model modification
    #print(f'custom_forward_pass_last_pp blocks: {len(self.h)}')

    past_length = 0
    past_key_values = tuple([None] * len(self.h))

    head_mask = self.get_head_mask(head_mask, self.config.num_layers)

    for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):
        outputs = block(
            hidden_states,
            layer_past=layer_past,
            attention_mask=attention_mask,
            head_mask=head_mask[i],
            use_cache=use_cache,
            output_attentions=output_attentions,
        )
    
        hidden_states = outputs[0]

    hidden_states = self.ln_f(hidden_states)

    hidden_states = hidden_states.view(output_shape)
    lm_head_output = lm_head(hidden_states)
    return lm_head_output


def custom_forward_pass_intermediate_pp(self, *args, **kwargs):
    print('custom_forward_pass_intermediate_pp')
    output_attentions = False
    output_hidden_states = False
    use_cache = False
    return_dict = False
    inputs_embeds = None
    attention_mask = None
    head_mask = None

    hidden_states = kwargs.pop('hidden_states')

    # hack, didn't realize the class was GPTNeoForCausalLM
    self = self.transformer

    past_length = 0
    past_key_values = tuple([None] * len(self.h))

    head_mask = self.get_head_mask(head_mask, self.config.num_layers)

    for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):
        outputs = block(
            hidden_states,
            layer_past=layer_past,
            attention_mask=attention_mask,
            head_mask=head_mask[i],
            use_cache=use_cache,
            output_attentions=output_attentions,
        )
    
        hidden_states = outputs[0]

    return hidden_states

def create_custom_forward_pass(forward_pass_type, block_only_partition):
    if forward_pass_type == "first":
        return functools.partialmethod(custom_forward_pass_first_pp, _lightrails_blocks=block_only_partition)
    if forward_pass_type == "last":
        return functools.partialmethod(custom_forward_pass_last_pp, _lightrails_blocks=block_only_partition)
    if forward_pass_type == "intermediate":
        return functools.partialmethod(custom_forward_pass_intermediate_pp, _lightrails_blocks=block_only_partition)

    assert False
    #return [custom_forward_pass_first_pp, custom_forward_pass_last_pp][pp_rank]


def modify_model_and_replace_forward_pass(gpt_neo_model, num_blocks, pp_rank, partitions):
    assert type(gpt_neo_model) == GPTNeoForCausalLM

    partition = partitions[pp_rank]
    block_only_partition = [block for block in partition if block >= 0 and block < num_blocks]
    print('partition', partition)
    if -1 in partition:
        forward_pass_type = "first"
    elif num_blocks in partition:
        forward_pass_type = "last"
    else:
        forward_pass_type = "intermediate"

    print(f'pp_rank {pp_rank} is type {forward_pass_type}, with blocks {block_only_partition}')

    remove_blocks_from_model(gpt_neo_model.transformer, block_only_partition)
    custom_fwd_pass = create_custom_forward_pass(
        forward_pass_type,
        block_only_partition,
    )

    gpt_neo_model.forward = custom_fwd_pass.__get__(
        gpt_neo_model,
        gpt_neo_model.__class__,
    )

#from transformers.models.gpt_neo import GPTNeoModel
#class PP_GPTNeoModel(GPTNeoModel):
#    def __init__(self, config):
#        super().__init__(config)

if True:
    
    checkpoint = "EleutherAI/gpt-neo-125m"
    config = AutoConfig.from_pretrained(checkpoint)
    #with init_empty_weights():
    #    model = AutoModelForCausalLM.from_config(config)
    #    model = model.eval()
    #    device_map = infer_auto_device_map(model)
    #    #print(model)
    #    #print(device_map)

    #stack = list(model.named_children())
    #zero = None
    #while stack:
    #    name, module = stack.pop()
    #    print(name)

    #    if name in ['transformer', 'h', '0']:
    #        stack.extend(list(module.named_children()))
    #        if name == '0':
    #            zero = module

    #device_map = {
    #    "transformer": "cpu",
    #    "lm_head": "cpu",
    #}

    #model = AutoModelForCausalLM.from_config(config)
    #model = dispatch_model(model, device_map=device_map)

    #from accelerate.utils.modeling import set_module_tensor_to_device

    #embedding = resolve_module(model, 'transformer.wte')
    #blocks = resolve_module(model, 'transformer.h')
    #print(blocks)

    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    input_sequence = "The quick brown fox jumps over the"
    input_tokens = tokenizer(input_sequence, return_tensors="pt")
    input_ids = input_tokens['input_ids']

    device_map = {
        "transformer": "cpu",
        "lm_head": "cpu",
    }

    #model_start = AutoModelForCausalLM.from_config(config)
    #model_start = dispatch_model(model_start, device_map=device_map)

    #model_end = AutoModelForCausalLM.from_config(config)
    #model_end = dispatch_model(model_end, device_map=device_map)

    model = AutoModelForCausalLM.from_config(config)
    model = dispatch_model(model, device_map=device_map)

    blocks = resolve_module(model, 'transformer.h')
    num_blocks = len(blocks)
    num_pp_rank = 12
    
    def partition(num_blocks=num_blocks, num_pp_rank=num_pp_rank):
        pp_rank_to_block = defaultdict(list)
        next_pp_rank = 0
        for block in range(num_blocks):
            pp_rank_to_block[next_pp_rank].append(block)

            if len(pp_rank_to_block[next_pp_rank]) >= (num_blocks // num_pp_rank):
                next_pp_rank += 1
        
        # TODO improve partition representation.
        # This currently means "pre-block work goes in pp0" and 
        # "post-block work goes in pp(-1)"
        pp_rank_to_block[0].append(-1)
        pp_rank_to_block[num_pp_rank-1].append(num_blocks)

        # Improve readability
        for _, blocks in pp_rank_to_block.items():
            blocks.sort()

        return pp_rank_to_block

    pp_rank_to_block = partition()
    print(pp_rank_to_block)

    pp_ranks = [model]
    while len(pp_ranks) < num_pp_rank:
        cpy = copy.deepcopy(model)
        pp_ranks.append(cpy)

    out = []
    for pp_rank, model in enumerate(pp_ranks):
        modify_model_and_replace_forward_pass(model, num_blocks, pp_rank=pp_rank, partitions=pp_rank_to_block)
        out.append(model)
    pp_ranks = out

    #modify_model_and_replace_forward_pass(model_start, num_blocks, pp_rank=0, partitions=pp_rank_to_block)
    #modify_model_and_replace_forward_pass(model_end, num_blocks, pp_rank=1, partitions=pp_rank_to_block)

    for _ in range(5):

        hidden_states = None
        output_shape = None
        lm_logits = None
        for pp_rank, model in enumerate(pp_ranks):
            if pp_rank == 0:
                hidden_states, output_shape = model.forward(
                    input_ids=input_ids,
                    return_dict=False,
                    output_attentions=False,
                    output_hidden_states=False,
                )
            elif pp_rank == len(pp_ranks) - 1:
                lm_logits = model.forward(
                    hidden_states=hidden_states,
                    output_shape=output_shape,
                )
            else:
                hidden_states = model.forward(
                    hidden_states=hidden_states,
                )

        next_token_logits = lm_logits[:, -1, :]

        processors = LogitsProcessorList()
        processors.append(LogitNormalization())

        next_token_scores = processors(input_ids, next_token_logits)
        #next_token_scores = logits_warper(input_ids, next_token_scores) # skipping
        probs = nn.functional.softmax(next_token_scores, dim=-1)
        next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
        decoded = tokenizer.decode(next_tokens)
        print(f"'{input_sequence}{decoded}'",)



    """
    logits_processor = self._get_logits_processor(
        generation_config=generation_config,
        input_ids_seq_length=input_ids_seq_length,
        encoder_input_ids=inputs_tensor,
        prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,
        logits_processor=logits_processor,
    )
    """
    


    #num_pp_stages = 2
    #blocks_start = blocks[:num_blocks//num_pp_stages]
    #blocks_end = blocks[-num_blocks//num_pp_stages:]

    #from transformers.models.gpt_neo import GPTNeoModel
    ## GPTNeoModel

    #embedding = resolve_module(model, 'transformer.wte')
    #blocks = resolve_module(model, 'transformer.h')
    #print(blocks)
    

if __name__ == '__main__':
    pass
    #physical_planner = SimplePhysicalPlanner()
    #placement_group = placement_group([{'GPU': 1}, {'GPU': 1}], strategy='STRICT_PACK')
    #logical_plan = gen_logical_plan()
    #
    #coordinator = Coordinator(
    #    logical_plan=logical_plan,
    #    pg=placement_group,
    #    planner=physical_planner,
    #)
